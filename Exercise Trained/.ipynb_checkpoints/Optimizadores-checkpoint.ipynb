{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef0753f-f3d9-4c2f-8661-6166d0c8ddd3",
   "metadata": {},
   "source": [
    "<img src=\"../img/Logo.png\" width=\"200\">\n",
    "\n",
    "# Aprendizaje por Imágenes\n",
    "## Optimizadores\n",
    "\n",
    "### Profesor: Jorge Calvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c48a02-3314-4d51-bbd2-b9b8e84de390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51c2c3-d839-4f64-bc64-6aa292eb81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset MNIST y preprocesar las imágenes\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# Reestructuramos y normalizamos (las imágenes se escalan a [0, 1])\n",
    "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "# Visualizar algunas imágenes de entrenamiento\n",
    "fig, axs = plt.subplots(2, 4, figsize=(8, 4))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(x_train[i])\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098d51e-7f86-4678-a315-e32b5e23ef21",
   "metadata": {},
   "source": [
    "### Explicación de Optimizadores\n",
    "\n",
    "#### AdaGrad:\n",
    "* Ventaja: Adapta bien la tasa de aprendizaje para parámetros con gradientes escasos (útil cuando algunas características visuales aparecen con menor frecuencia).\n",
    "* Desventaja: La tasa de aprendizaje puede disminuir demasiado con el tiempo.\n",
    "\n",
    "#### RMSProp:\n",
    "\n",
    "* Ventaja: Mantiene una tasa de aprendizaje estable mediante un promedio móvil de los gradientes, lo que es beneficioso en escenarios con funciones de pérdida muy variables, como es frecuente en redes convolucionales.\n",
    "* Desventaja: Requiere ajustar el hiperparámetro de decaimiento (𝛾).\n",
    "\n",
    "#### Adam:\n",
    "\n",
    "* Ventaja: Combina la estabilidad de RMSProp y la eficiencia del momentum, lo que resulta en una convergencia rápida y robusta en una amplia variedad de problemas de visión artificial.\n",
    "* Desventaja: Aunque suele funcionar bien en la mayoría de los casos, en algunos escenarios específicos puede requerir ajustes finos de la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef4297-ab02-4c3f-b68f-75513fac2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para construir la arquitectura del modelo\n",
    "def build_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Diccionario con los optimizadores a comparar\n",
    "optimizers_dict = {\n",
    "    \"Adam\": optimizers.Adam(learning_rate=0.001),\n",
    "    \"AdaGrad\": optimizers.Adagrad(learning_rate=0.001),\n",
    "    \"RMSProp\": optimizers.RMSprop(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e2468-e98c-4e70-bfd6-f37d4096dcaa",
   "metadata": {},
   "source": [
    "# Fórmulas de Cross Entropy y Sparse Categorical Cross Entropy\r\n",
    "\r\n",
    "### Categorical Cross Entropy\r\n",
    "\r\n",
    "La función de pérdida de **Categorical Cross Entropy** se define como:\r\n",
    "\r\n",
    "$$\r\n",
    "L = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\r\n",
    "$$\n",
    "\n",
    "donde:\r\n",
    "- \\( C \\) es el número\n",
    " clases.\r\n",
    "- \\( y_i \\) es la etiqueta verdadera para la clase \\( i \\) (en formato one-hot, es decir, \\( y_i = 1 \\) para la clase correcta y \\( 0 \\) para\n",
    "s demás).\r\n",
    "- \\( \\hat{y}_i \\) es la probabilidad predicha para la c\n",
    "\n",
    "#### Ventajas:\n",
    "\n",
    "Directamente optimizada para clasificación.\n",
    "Penaliza fuertemente las predicciones incorrectas.\n",
    "La salida softmax genera una distribución de probabilidad que se alinea con la función de pérdida.\n",
    "\n",
    "* Aplicación en MNIST:\n",
    "Es la función de pérdida más utilizada en tareas de clasificación multiclase, ya que proporciona un gradiente bien definido para la actualización de los pesos.lase \\( i \\).\r\n",
    "\r\n",
    "### Sparse Categorical Cross Entropy\r\n",
    "\r\n",
    "La función de pérdida de **Sparse Categorical Cross Entropy** se define como:\r\n",
    "\r\n",
    "$$\r\n",
    "L = -\\log\\left(\\hat{y}_{y_{\\text{true}}}\\right)\r\n",
    "$$\r\n",
    "\r\n",
    "donde:\r\n",
    "- \\( \\hat{y}_{y_{\\text{true}}} \\) es la probabilidad predicha para la clase correcta.\r\n",
    "- Las etiquetas verdaderas se representan como enteros (por ejemplo, \\( 3 \\) en lugar de \\(\n",
    "\n",
    "#### Ventajas:\n",
    "\n",
    "Menos consumo de memoria y computacionalmente más eficiente cuando se tienen muchas clases.\n",
    "Evita el paso de transformación a one-hot, lo cual simplifica el preprocesamiento.\n",
    "\n",
    "* Aplicación en MNIST:\n",
    "Debido a que MNIST tiene 10 clases y las etiquetas pueden representarse fácilmente como enteros, es común usar sparse categorical cross entropy para simplificar el código y la preparación de datos.[0,0,0,1,0,0,0,0,0,0]\\)).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961f943-e673-4753-b662-29aff21aceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenaremos los historiales de entrenamiento para cada optimizador\n",
    "histories = {}\n",
    "\n",
    "for opt_name, opt in optimizers_dict.items():\n",
    "    print(f\"Entrenando con el optimizador: {opt_name}\")\n",
    "    model = build_model()\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',  # Utilizamos sparse ya que las etiquetas son enteros\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=1)\n",
    "    histories[opt_name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fae24-9aaf-4ed4-bc45-edc441e4b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las curvas de entrenamiento y validación para precisión y pérdida\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Curva de precisión\n",
    "plt.subplot(1, 2, 1)\n",
    "for opt_name, history in histories.items():\n",
    "    plt.plot(history.history['accuracy'], label=f'{opt_name} - Entrenamiento')\n",
    "    plt.plot(history.history['val_accuracy'], label=f'{opt_name} - Validación', linestyle='--')\n",
    "plt.title('Evolución de la Precisión')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "\n",
    "# Curva de pérdida\n",
    "plt.subplot(1, 2, 2)\n",
    "for opt_name, history in histories.items():\n",
    "    plt.plot(history.history['loss'], label=f'{opt_name} - Entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label=f'{opt_name} - Validación', linestyle='--')\n",
    "plt.title('Evolución de la Pérdida')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd733cc2-ce75-469e-bb3c-22a7eca2d100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
