{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "<div align=\"center\">\n",
    "<h3>Jorge Calvo</h3>\n",
    "    \n",
    "<h1>Funciones de activación</h1>\n",
    "    <h2>Redes neuronales</h2>\n",
    "\n",
    "<h3>Sígueme en - <a href=\"https://www.linkedin.com/in/jorgecalvomartin/\">LinkedIn</a>&nbsp; <a href=\"https://twitter.com/jorgemcalvo\">Twitter</a>&nbsp; <a href=\"https://www.europeanvalley.es/noticias/\">Blog</a>&nbsp;</h3>\n",
    "</div>\n",
    "    \n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Licencia de Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />Este obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">licencia de Creative Commons Reconocimiento-NoComercial-CompartirIgual 4.0 Internacional</a>.\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, random\n",
    "import sympy as sym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from IPython.display import display, Math, Latex, HTML\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función Sigmoide\n",
    "La razón principal por la que usamos la función sigmoide es porque existe entre (0 a 1). Por lo tanto, se usa especialmente para modelos en los que tenemos que predecir la probabilidad como un resultado. Dado que la probabilidad de cualquier cosa existe solo entre el rango de 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la función sigmoide con lambda , donde lambda[0] es la sigmoide y lambda[1] es su derivada\n",
    "sigm = (lambda x:1/(1+np.e**(-x)),lambda x:np.exp(-x)/(np.exp(-x)+1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latex\n",
    "display(Math(r'sigmoide(x) = \\frac{1}{1+e^{-x}} '))\n",
    "display(Math(r\"$$sigmoide'(x) = \\frac{e^{-x}}{(e^{-x}+1)^2}\"))\n",
    "\n",
    "v = np.linspace(-5,5,100)\n",
    "plt.figure(1,figsize=(15,10))\n",
    "plt.subplot(221)\n",
    "plt.plot(v,sigm[0](v))\n",
    "plt.title(\"Funcion Sigmoide\")\n",
    "plt.subplot(222)\n",
    "plt.plot(v,sigm[1](v), \"--\", color=\"red\")\n",
    "plt.title(\"Derivada función Sigmoide\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Características:\n",
    "* Saturación: Para valores muy altos o muy bajos de x, la sigmoide se aplana (se aproxima a 1 o 0, respectivamente). Esto implica que para entradas extremas, el gradiente se vuelve muy pequeño (problema del vanishing gradient), dificultando la actualización de los pesos.\n",
    "* No centrada en cero: Esto puede ocasionar que durante el entrenamiento la propagación del gradiente sea menos eficiente, ya que las salidas siempre son positivas.\n",
    "\n",
    "#### Impacto en los filtros:\n",
    "* Al aplicar la sigmoide, los filtros de la convolución pasan sus respuestas a un rango limitado, lo cual puede \"suavizar\" la información y, en algunos casos, hacer que se pierdan diferencias sutiles entre características.\n",
    "* Además, la saturación puede hacer que ciertos filtros apenas contribuyan al aprendizaje si sus respuestas caen en las zonas planas de la función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función Tangente hiperbólica o Gaussiana\n",
    "Es una función similar a la Sigmoide pero produce salidas en escala de [-1, +1]. Además, es una función continua. En otras palabras, la función produce resultados para cada valor de x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tangente Hiperbólica y su derivada\n",
    "cosh = (lambda x: (np.e**(x) + np.e**(-x))/2) #Saco la función de coseno hiperbólico para la derivada de la tangh\n",
    "tanh = (lambda x: (np.e**(x) - np.e**(-x))/(np.e**(x) + np.e**(-x)), lambda x: 1/cosh(x)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latex\n",
    "display(Math(r'cosh(x) = \\frac{e^{x} + e^{-x}}{2}'))\n",
    "display(Math(r'tanh(x) = \\frac{e^{x} - e^{-x}}{e^{-x} + e^{-x}} '))\n",
    "display(Math(r\"'tanh(x) = \\frac{1}{\\cosh^2{x}} \"))\n",
    "\n",
    "v = np.linspace(-5,5,100)\n",
    "plt.figure(1,figsize=(15,10))\n",
    "plt.subplot(331)\n",
    "plt.plot(v,cosh(v))\n",
    "plt.title(\"Funcion Coseno hiperbólico\")\n",
    "plt.subplot(332)\n",
    "plt.plot(v,tanh[0](v))\n",
    "plt.title(\"Función Tangente hiperbólica\")\n",
    "plt.subplot(333)\n",
    "plt.plot(v,tanh[1](v), \"--\", color=\"red\")\n",
    "plt.title(\"Derivada función Tangente hiperbólica\")\n",
    "plt.savefig('images\\sigmoide.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Características:\n",
    "* Centrada en cero: Esto significa que los valores de activación están balanceados entre positivos y negativos, lo que ayuda a una propagación del gradiente más simétrica y puede conducir a una convergencia más rápida.\n",
    "* Saturación similar a la sigmoide: Aunque mejora el tema del centrado, la tanh sigue saturándose para valores extremos, lo que puede disminuir el gradiente y ralentizar el aprendizaje en esos rangos.\n",
    "#### Impacto en los filtros:\n",
    "* La tanh permite que los filtros resalten tanto las activaciones positivas como las negativas. Esto puede ser beneficioso cuando la presencia o ausencia (incluso con signo negativo) de ciertas características es relevante.\n",
    "* Sin embargo, la saturación sigue siendo un factor a considerar, ya que puede limitar la sensibilidad de algunos filtros ante variaciones sutiles en la entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función RELU (Rectified Lineal Unit)\n",
    "ReLU es la función de activación más utilizada en el mundo en este momento. Desde entonces, se utiliza en casi todas las redes neuronales convolucionales o el aprendizaje profundo.\n",
    "Como puedes ver, ReLU está medio rectificado (desde abajo). f(z) es cero cuando z es menor que cero y f(z) es igual a z cuando z es superior o igual a cero.\n",
    "Es una función usada en las capas ocultas de nuestra red neuronal, NO en las de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relu Rectified Lineal Unit\n",
    "relu = (lambda x: np.maximum(0,x), lambda x: 1. * (x > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latex\n",
    "display(Math(r'relu(x) = \\max(0,x) '))\n",
    "display(Math(r\"'relu(x) = 1.(x>0) \"))\n",
    "\n",
    "v = np.linspace(-5,5,100)\n",
    "plt.figure(1,figsize=(15,10))\n",
    "plt.subplot(221)\n",
    "plt.plot(v,relu[0](v))\n",
    "plt.title(\"Funcion RELU\")\n",
    "plt.subplot(222)\n",
    "plt.plot(v,relu[1](v), \"--\", color=\"red\")\n",
    "plt.title(\"Derivada función RELU\")\n",
    "plt.savefig('images\\sigmoide.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Características:\n",
    "* No saturante para x>0: La derivada es constante (1) en la región positiva, lo que ayuda a que el gradiente fluya de manera eficiente durante el entrenamiento.\n",
    "* Sparsity: Al establecer a cero todos los valores negativos, se produce un efecto de \"sparse activation\" (activaciones dispersas). Esto puede ayudar a enfocar el aprendizaje en las características más relevantes.\n",
    "* Problema de neuronas muertas: Si un filtro (o neurona) se ve constantemente con entradas negativas, puede dejar de actualizarse (quedarse \"muerta\") y no aprender nada.\n",
    "\n",
    "#### Impacto en los filtros:\n",
    "* Con ReLU, los filtros responden de manera más directa y sin saturación a los patrones relevantes presentes en la imagen.\n",
    "* La eliminación de valores negativos puede ayudar a enfatizar la presencia de características, pero a costa de perder información sobre la \"dirección\" (signo) de la activación, lo que en algunos contextos puede ser una limitación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky-ReLU\n",
    "Leaky-ReLU es una mejora del valor predeterminado principal de ReLU, en el sentido de que puede manejar los valores negativos bastante bien, pero aún presenta no linealidad.\n",
    "\n",
    "$ LRelu(x)=max(0.01x,x) $\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "     LRelu'(x) = \\left\\{\n",
    "\t       \\begin{array}{ll}\n",
    "\t\t 1      & \\mathrm{si\\ } x > 0 \\\\\n",
    "\t\t 0.01 & \\mathrm{si\\ } x < 0 \\\\\n",
    "\t       \\end{array}\n",
    "\t     \\right.\n",
    "   \\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrelu = (lambda x: np.maximum(0.01*x,x), lambda x: 1. if (x > 0) else 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(15,10))\n",
    "for u in range(1, 5):\n",
    "    inputs = [x for x in range(-5, 5)]\n",
    "    outputs = [lrelu[0](x) for x in inputs]\n",
    "    output_prime = [lrelu[1](x) for x in inputs]\n",
    "    plt.subplot(221)\n",
    "    plt.grid(visible=True)\n",
    "    plt.title(\"LRELU\")\n",
    "    plt.plot(inputs, outputs)\n",
    "    plt.subplot(222)\n",
    "    plt.grid(visible=True)\n",
    "    plt.title(\"Derivada LRELU\")\n",
    "    plt.plot(inputs, output_prime, \"--\", color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de softmax:\n",
    "La función softmax toma un vector de logits y calcula el exponente de cada uno (ajustado restando el máximo para evitar números muy grandes) y luego divide cada valor por la suma total de esos exponentes. Esto convierte los logits en una distribución de probabilidades que suma 1.\n",
    "\n",
    "**Simulación de logits:**\n",
    "El vector logits simula las salidas de la última capa (por ejemplo, después de aplanar la salida de una red convolucional). Estos valores no representan probabilidades hasta que se les aplica softmax.\n",
    "\n",
    "**Cálculo de probabilidades:**\n",
    "Aplicamos la función softmax sobre los logits para obtener el vector probabilidades, que contiene valores entre 0 y 1 para cada clase.\n",
    "\n",
    "**Visualización:**\n",
    "Se generan dos gráficos de barras:\n",
    "\n",
    "Izquierda: Muestra los logits originales.\n",
    "Derecha: Muestra las probabilidades obtenidas con softmax, ilustrando cómo se distribuyen los \"pesos\" de cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Calcula la función softmax de un vector de logits.\n",
    "    Se resta el máximo de los logits para mayor estabilidad numérica.\n",
    "    \"\"\"\n",
    "    exps = np.exp(logits - np.max(logits))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "# Supongamos que estos son los logits (salidas sin normalizar) de la última capa de una red convolucional.\n",
    "# Por ejemplo, para una clasificación en 5 clases.\n",
    "logits = np.array([2.5, 1.0, 0.5, -1.0, 3.0])\n",
    "probabilidades = softmax(logits)\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Probabilidades (Softmax):\", probabilidades)\n",
    "\n",
    "# Visualización\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Gráfico de los logits\n",
    "clases = np.arange(len(logits))\n",
    "axes[0].bar(clases, logits, color='skyblue')\n",
    "axes[0].set_title('Logits (sin Softmax)')\n",
    "axes[0].set_xlabel('Clase')\n",
    "axes[0].set_ylabel('Valor del logit')\n",
    "axes[0].set_xticks(clases)\n",
    "\n",
    "# Gráfico de las probabilidades resultantes del softmax\n",
    "axes[1].bar(clases, probabilidades, color='salmon')\n",
    "axes[1].set_title('Probabilidades (Softmax)')\n",
    "axes[1].set_xlabel('Clase')\n",
    "axes[1].set_ylabel('Probabilidad')\n",
    "axes[1].set_xticks(clases)\n",
    "\n",
    "fig.suptitle('Ejemplo visual de la función Softmax en una red convolucional')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
