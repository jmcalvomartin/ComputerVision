{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d32254c-16b6-47b2-b484-41c03640e1ea",
   "metadata": {},
   "source": [
    "<img src=\"../img/UAX.png\" width=\"300\">\n",
    "\n",
    "### Profesor: Jorge Calvo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec40c3-af51-4813-9b25-31d180451c8c",
   "metadata": {},
   "source": [
    "## Autoencoders Variacionales (VAE)\n",
    "\n",
    "Los autoencoders variacionales (VAE) son una extensión de los autoencoders convencionales que se utilizan para aprender una representación latente de los datos de entrada. A diferencia de los autoencoders tradicionales, los VAE tienen una estructura probabilística que les permite generar nuevas muestras de datos similares a las del conjunto de entrenamiento. Esta característica los hace útiles en tareas como la generación de imágenes, síntesis de texto y otras aplicaciones donde se necesita la generación de nuevos datos similares a los ya existentes.\n",
    "\n",
    "<img src=\"../img/vae.png\" width=\"1000\">\n",
    "\n",
    "<img src=\"../img/z.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce7313",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Arquitectura Básica\n",
    "\n",
    "1. **Encoder**  \n",
    "   - Toma una entrada \\(x\\) (por ejemplo, una imagen) y la proyecta a un espacio latente de baja dimensión.  \n",
    "   - A diferencia del autoencoder clásico, el encoder no produce un solo vector latente, sino dos vectores:\n",
    "   \n",
    "     - $\\boldsymbol{\\mu}(x) $: el **vector de medias**.\n",
    "     - $ \\boldsymbol{\\sigma}(x) $: el **vector de desviaciones estándar** (o log-varianzas).\n",
    "\n",
    "2. **Cuello de Botella (Latent Space)**  \n",
    "   - En lugar de un punto determinístico, representamos cada entrada como una **distribución gaussiana** $ q(z|x) = \\mathcal{N}\\bigl(z;\\,\\boldsymbol{\\mu}(x),\\,\\mathrm{diag}(\\boldsymbol{\\sigma}^2(x))\\bigr)$.  \n",
    "   - Para muestrear un punto $ z $ de esta distribución y permitir el paso de gradiente, aplicamos el **truco de reparametrización**:\n",
    "     $\n",
    "       z = \\underbrace{\\boldsymbol{\\mu}(x)}_{\\text{media}} + \n",
    "           \\underbrace{\\boldsymbol{\\sigma}(x)}_{\\text{desviación}} \\;\\odot\\;\n",
    "           \\underbrace{\\epsilon}_{\\mathcal{N}(0,I)}\n",
    "     $\n",
    "   - Aquí $\\epsilon\\sim\\mathcal{N}(0,I)$ añade estocasticidad, pero la parte diferenciable (medias y desviaciones) permite backpropagation.\n",
    "\n",
    "3. **Decoder**  \n",
    "   - Toma la muestra $ z $ y genera una reconstrucción $ \\hat{x}$.  \n",
    "   - Aprende la función inversa que, idealmente, recupera la distribución original de \\(x\\).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ¿Por qué Media y Desviación en el Bottleneck?\n",
    "\n",
    "- **Media $\\mu $**:  \n",
    "  Describe el “centro” de la distribución latente para cada $x $.  \n",
    "- **Desviación $\\sigma $**:  \n",
    "  Indica cuánta variabilidad permitimos alrededor de esa media.  \n",
    "\n",
    "Esta parametrización probabilística aporta dos beneficios clave:\n",
    "\n",
    "1. **Regularización Suave**: Evita que diferentes entradas se colapsen a un mismo punto latente; en lugar de eso, cada $x$ ocupa una pequeña “nube” en el espacio latente.  \n",
    "2. **Generación de Muestras Nuevas**: Podemos muestrear distintos $\\epsilon$ y generar múltiples $z$ para un mismo $x$, o incluso muestrear de la distribución estándar $\\mathcal{N}(0,I)$ para crear datos completamente nuevos y coherentes.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Función de Pérdida Combinada\n",
    "\n",
    "La función de pérdida de un VAE consta de dos términos:\n",
    "\n",
    "1. **Reconstrucción** $\\displaystyle \\mathcal{L}_{\\mathrm{rec}} = -\\mathbb{E}_{q(z|x)}\\bigl[\\log p(x|z)\\bigr]$\n",
    "   — Mide cuán bien el decoder puede reconstruir $x$ a partir de $z$.  \n",
    "   — Equivalente a la pérdida habitual de autoencoder (MSE, cross-entropy, …).\n",
    "\n",
    "2. **Divergencia KL** $\\displaystyle \\mathcal{L}_{\\mathrm{KL}} = D_{\\mathrm{KL}}\\bigl(q(z|x)\\parallel p(z)\\bigr)$ \n",
    "   — Mide cuánto difiere la distribución latente $q(z|x)$ de la **prior** $p(z)$ (normalmente \\($\\mathcal{N}(0,I)$)).  \n",
    "   — Fórmula cerrada para Gaussianas:\n",
    "   $\n",
    "     D_{\\mathrm{KL}}\\bigl(\\mathcal{N}(\\mu,\\sigma^2)\\,\\|\\,\\mathcal{N}(0,1)\\bigr)\n",
    "     = \\tfrac12 \\sum_{i=1}^d \\Bigl(\\mu_i^2 + \\sigma_i^2 - \\log\\sigma_i^2 - 1\\Bigr)\n",
    "   $\n",
    "\n",
    "El **objetivo total** que minimizamos es:\n",
    "$\n",
    "  \\mathcal{L}_{\\mathrm{VAE}}(x) \n",
    "  = \\underbrace{\\mathcal{L}_{\\mathrm{rec}}}_{\\text{Calidad de reconstrucción}}\n",
    "  \\;+\\; \n",
    "  \\underbrace{\\mathcal{L}_{\\mathrm{KL}}}_{\\text{Regularización del latente}}\n",
    "$\n",
    "- El término KL fuerza a las medias $\\mu(x)$ y desviaciones $\\sigma(x)$ a aproximarse a $(0,1)$, evitando “sobreajuste” del espacio latente.  \n",
    "- La reconstrucción asegura que la decodificación sea fiel a los datos originales.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Resumen Didáctico\n",
    "\n",
    "1. **Encoder → $(\\mu, \\sigma)$**: Cada imagen $x$ se mapea a una **distribución** latente, no a un punto fijo.  \n",
    "2. **Reparametrización**:  \n",
    "   $\n",
    "     z = \\mu + \\sigma \\,\\odot\\, \\epsilon \\quad,\\quad \\epsilon\\sim\\mathcal{N}(0,I)\n",
    "   $\n",
    "   Permite pasar gradientes durante el muestreo.  \n",
    "3. **Decoder → $\\hat{x}$**: Reconstruye a partir de $z$.  \n",
    "4. **Pérdida = Reconstrucción + KL**:  \n",
    "   - **Reconstrucción** garantiza fidelidad.  \n",
    "   - **KL** impone una distribución latente estándar, favoreciendo interpolación y generación de muestras.\n",
    "\n",
    "Con este esquema tus alumnos pueden entender por qué **media** y **desviación** en el cuello de botella, y cómo la combinación de **reconstrucción** y **divergencia KL** produce un modelo probabilístico que **aprende** una representación latente continua, estructurada y generativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b236b1",
   "metadata": {},
   "source": [
    "\n",
    "### Ejemplo Numérico Paso a Paso de un VAE\n",
    "\n",
    "\n",
    "#### 1. Salidas del Encoder\n",
    "\n",
    "Supongamos que el encoder, aplicado a una entrada $x$, produce:\n",
    "\n",
    "- Vector de medias:\n",
    "  $\n",
    "    \\boldsymbol{\\mu} = [\\,1.0,\\;2.0\\,]\n",
    "  $\n",
    "- Vector de log-varianzas:\n",
    "  $\n",
    "    \\log \\boldsymbol{\\sigma}^2 = [\\,-1.386,\\;-3.219\\,]\n",
    "  $\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Cálculo de la desviación $\\boldsymbol{\\sigma}$\n",
    "\n",
    "Recordemos que\n",
    "$\n",
    "  \\sigma_i = \\exp\\!\\bigl(\\tfrac12 \\log \\sigma_i^2\\bigr).\n",
    "$\n",
    "\n",
    "- Para $i=1$:\n",
    "  $\n",
    "    \\log \\sigma_1^2 = -1.386 \n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    \\sigma_1 = \\exp(-1.386/2) = \\exp(-0.693) \\approx 0.50\n",
    "  $\n",
    "- Para \\(i=2\\):\n",
    "  $\n",
    "    \\log \\sigma_2^2 = -3.219 \n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    \\sigma_2 = \\exp(-3.219/2) = \\exp(-1.609) \\approx 0.20\n",
    "  $\n",
    "\n",
    "Así,\n",
    "$\n",
    "  \\boldsymbol{\\sigma} = [\\,0.5,\\;0.2\\,].\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Muestreo con el truco de reparametrización\n",
    "\n",
    "Tomamos\n",
    "$\n",
    "  \\epsilon \\sim \\mathcal{N}(0,I),\n",
    "  \\quad\n",
    "  z = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\,\\odot\\, \\epsilon.\n",
    "$\n",
    "\n",
    "Si elegimos por ejemplo\n",
    "$\n",
    "  \\epsilon = [\\,1.0,\\;-1.0\\,],\n",
    "$\n",
    "entonces\n",
    "\n",
    "$\n",
    "  z_1 = 1.0 + 0.5 \\times 1.0 = 1.5,\n",
    "  \\quad\n",
    "  z_2 = 2.0 + 0.2 \\times (-1.0) = 1.8,\n",
    "$\n",
    "y obtenemos\n",
    "$\n",
    "  \\mathbf{z} = [\\,1.5,\\;1.8\\,].\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Cálculo de la Divergencia KL\n",
    "\n",
    "Para cada dimensión $i$, la contribución es\n",
    "\n",
    "$\n",
    "  D_{KL}\\bigl(\\mathcal{N}(\\mu_i,\\sigma_i^2)\\,\\|\\;\\mathcal{N}(0,1)\\bigr)\n",
    "  = \\tfrac12\\bigl(\\mu_i^2 + \\sigma_i^2 - \\log \\sigma_i^2 - 1\\bigr).\n",
    "$\n",
    "\n",
    "- **Dimensión 1**:\n",
    "  $\n",
    "    \\mu_1^2 = 1^2 = 1,\\quad\n",
    "    \\sigma_1^2 = 0.25,\\quad\n",
    "    \\log\\sigma_1^2 = -1.386,\n",
    "  $\n",
    "  $\n",
    "    d_1\n",
    "    = \\tfrac12\\bigl(1 + 0.25 - (-1.386) - 1\\bigr)\n",
    "    = \\tfrac12(1.25 + 1.386 - 1)\n",
    "    = 0.818\n",
    "  $\n",
    "\n",
    "- **Dimensión 2**:\n",
    "  $\n",
    "    \\mu_2^2 = 4,\\quad\n",
    "    \\sigma_2^2 = 0.04,\\quad\n",
    "    \\log\\sigma_2^2 = -3.219,\n",
    "  $\n",
    "  $\n",
    "    d_2\n",
    "    = \\tfrac12\\bigl(4 + 0.04 - (-3.219) - 1\\bigr)\n",
    "    = \\tfrac12(3.04 + 3.219)\n",
    "    = 3.130\n",
    "  $\n",
    "\n",
    "Sumando ambas:\n",
    "$\n",
    "  D_{KL} = d_1 + d_2 \\approx 0.818 + 3.130 = 3.948.\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### Resumen de Valores\n",
    "\n",
    "- $\\boldsymbol{\\mu} = [1.0,\\;2.0]$\n",
    "- $\\log \\boldsymbol{\\sigma}^2 = [-1.386,\\;-3.219]$  \n",
    "- $\\boldsymbol{\\sigma} = [0.5,\\;0.2]$  \n",
    "- $\\epsilon = [1.0,\\;-1.0]$  \n",
    "- $\\mathbf{z} = [1.5,\\;1.8]$  \n",
    "- $D_{KL} \\approx 3.948$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers, models, losses, backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.models import Model, load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144a5a5-2059-4b95-b483-2d961ca5ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Visualizar algunas imágenes de entrenamiento\n",
    "fig, axs = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef047e3-991b-486b-855f-b31ff7d5975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747d346-a9c8-4282-be2e-20627b7ec0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "encoder_input = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, (3, 3), padding=\"same\", activation='relu')(encoder_input)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0949d-f417-4dde-b2d7-bd6a2abd3fbf",
   "metadata": {},
   "source": [
    "### Codificación Probabilística:\n",
    "\n",
    "El encoder toma una entrada x y produce dos salidas: la media (μ) y la varianza logarítmica (log(𝜎²))\n",
    "Estas salidas definen una distribución gaussiana N(𝜇,𝜎²) en el espacio latente.\n",
    "Un punto z en el espacio latente se obtiene muestreando de esta distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea04625-a875-49d8-a0ad-0e4c6763361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capa de muestreo\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75aea5-c73b-4da1-90a2-14b3800e0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "decoder_input = layers.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7*7*64, activation='relu')(decoder_input)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "decoded = layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90ea7b-9d3f-483e-9e13-b5eb17a76978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construir VAE\n",
    "encoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "decoder = models.Model(decoder_input, decoded, name=\"decoder\")\n",
    "vae_input = encoder_input\n",
    "vae_output = decoder(z)\n",
    "vae = models.Model(vae_input, vae_output, name=\"vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582114cf-dc55-4681-b35f-5363270ec971",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(vae, to_file='./vae.png', show_shapes=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266392a-09f7-4354-bb41-00d0cf5f0daa",
   "metadata": {},
   "source": [
    "### Pérdida del VAE:\n",
    "\n",
    "La función de pérdida de un VAE tiene dos componentes:\n",
    "\n",
    "* Pérdida de Reconstrucción: Mide qué tan bien el decoder puede reconstruir la entrada original a partir del punto muestreado en el espacio latente.\n",
    "\n",
    "* Pérdida KL (Kullback-Leibler): Mide la divergencia entre la distribución aprendida por el encoder y una distribución gaussiana estándar. Esta regularización asegura que las representaciones latentes sean continuas y bien distribuidas.\n",
    "La pérdida total se define como:\n",
    "\n",
    "\n",
    "#### Perdida Total = Perdida Reconstrucción + Pérdida KL\n",
    "\n",
    " \n",
    "$\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\text{KL}} = -\\frac{1}{2} \\sum_{i=1}^{d} \\left( 1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2 \\right)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "<img src=\"../img/latent.png\" width=\"1000\">\n",
    "\n",
    "<img src=\"../img/Kullback-Leibler.png\" width=\"400\">\n",
    "\n",
    "https://www.europeanvalley.es/noticias/el-espacio-latente-en-la-ia/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b45bef-49f0-4ce0-b2bd-8fbd59c50297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_sample(x, mean, var):\n",
    "    return ((1 / (np.sqrt(2 * np.pi * var))) * np.exp(-0.5 * (x - mean) ** 2 / var))\n",
    "\n",
    "# Setting mean, variance and x\n",
    "mean = [0.0, 0.0, 0.0, -2.0]\n",
    "var = [0.2, 1.0, 5.0, 0.5]\n",
    "x_range = np.arange(-5.0, 5.0, 0.05)\n",
    "curves = []\n",
    "\n",
    "# Computing distributions from each mean-var combiantions\n",
    "for i in range(len(mean)):\n",
    "    crv = [normal_sample(x, mean[i], var[i]) for x in x_range]\n",
    "    curves.append(crv)\n",
    "\n",
    "# Visualize curves\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "for i in range(len(curves)):\n",
    "    plt.plot(x_range, curves[i], \n",
    "             label=f\"$\\mu$={mean[i]}, $\\sigma^2$={var[i]}\")\n",
    "\n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce89e3-f137-491d-9e91-8bd8975d4cab",
   "metadata": {},
   "source": [
    "$\n",
    "f(x)= \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp^{-\\frac{(x-\\mu)²}{2\\sigma²}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae95c8-49d4-4b78-82ee-a375ef64d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1c9cb-2d77-492b-adf4-52a8a15cb58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63198d52-faa4-4678-9b6f-f5b62825f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizar resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
