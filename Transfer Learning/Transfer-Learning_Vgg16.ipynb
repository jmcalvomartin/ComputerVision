{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89639729-856f-4195-a27a-c02a3e76aadb",
   "metadata": {},
   "source": [
    "<img src=\"../img/Logo.png\" width=\"200\">\n",
    "\n",
    "# Aprendizaje por Imágenes\n",
    "# Transferencia de aprendizaje (Modelos pre-entrenados)\n",
    "\n",
    "### Profesor: Jorge Calvo\n",
    "\n",
    "## Modelo Vgg16\n",
    "\n",
    "Imagina que quieres enseñar a una computadora a reconocer diferentes objetos en imágenes, como perros, gatos, coches, etc. Para hacerlo, necesitarías miles y miles de imágenes etiquetadas, es decir, imágenes que ya tienen la información de qué objeto se muestra en cada una.\n",
    "\n",
    "Entrenar un modelo de reconocimiento de objetos desde cero puede llevar mucho tiempo y recursos computacionales. Aquí es donde entra en juego un modelo preentrenado, como VGG16.\n",
    "\n",
    "Un modelo preentrenado es un modelo de aprendizaje profundo que ya ha sido entrenado en un conjunto masivo de imágenes. En el caso de VGG16, ha sido entrenado en el conjunto de datos ImageNet (https://www.image-net.org/), que contiene millones de imágenes de diferentes categorías.\n",
    "\n",
    "Durante el entrenamiento, VGG16 aprendió a reconocer características comunes en las imágenes, como bordes, formas y texturas, y a asociar esas características con las etiquetas de las imágenes. Esto significa que VGG16 ya ha aprendido una representación útil y general de las características visuales en las imágenes.\n",
    "\n",
    "Cuando utilizas un modelo preentrenado como VGG16, puedes aprovechar ese conocimiento previo para resolver tareas específicas sin tener que entrenar desde cero. En lugar de entrenar todo el modelo, puedes tomar el modelo preentrenado y adaptarlo a tu problema específico.\n",
    "\n",
    "### Estructura\n",
    "<img src=\"../img/vgg.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc306c9-08fe-4832-b6e1-a72e816ec0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f53a182",
   "metadata": {},
   "source": [
    "https://keras.io/api/applications/vgg/#vgg16-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821684c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo preentrenado VGG16\n",
    "model = VGG16(weights='imagenet')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862aa0cf-0284-4bae-ba3e-6799501e39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='./red_convolucional.png', show_shapes=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14821ab6-4d6e-4d6d-bb24-8b5517b12940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y preprocesar una imagen de ejemplo\n",
    "img_path = '../images/camion.jpg'\n",
    "\n",
    "#Le damos el tamaño admitido por Vgg16\n",
    "img = image.load_img(img_path, target_size=(224, 224)) #Cargamos la imagen con el tamaño admitido por vgg16\n",
    "x = image.img_to_array(img) #Convertimos la imagen en un array\n",
    "x = np.expand_dims(x, axis=0) #Le añadimos la dimensión del batch\n",
    "x = preprocess_input(x) #Le pasamos el procesamiento particular de vgg16\n",
    "\n",
    "print(\"La imagen tiene la forma \",x.shape)\n",
    "\n",
    "# Realizar la clasificación de la imagen\n",
    "predictions = model.predict(x)\n",
    "#Usamos la propiedad decoded_predictions para obetener la mejor probabilidad\n",
    "decoded_predictions = decode_predictions(predictions, top=5)[0]\n",
    "\n",
    "print(decoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26b07a-d3bd-4940-b373-0104bf507e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir las predicciones\n",
    "for pred in decoded_predictions:\n",
    "    print(f\"{pred[1]}: {pred[2]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51e292-12a0-44c6-9cac-9cf51943a5dc",
   "metadata": {},
   "source": [
    "## Las activaciones de cada capa convolucional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea90469-1b4b-4a3a-b80a-56e1683e9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las salidas de las diferentes capas por orden\n",
    "#layer_outputs = [layer.output for layer in model.layers[1:19]]  # Capas convolucionales hasta block4_pool\n",
    "#activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "\n",
    "# Visualizar las salidas de las capas por nombres\n",
    "selected_layers = ['block1_conv1', 'block1_conv2']\n",
    "layer_outputs = [model.get_layer(layer_name).output for layer_name in selected_layers]\n",
    "activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "\n",
    "activations = activation_model.predict(x)\n",
    "\n",
    "images_per_row = 8\n",
    "\n",
    "for layer_name, layer_activation in zip(selected_layers, activations):\n",
    "    num_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # Crear una cuadrícula de imágenes para visualizar las activaciones\n",
    "    rows = num_features // images_per_row\n",
    "    display_grid = np.zeros((size * rows, images_per_row * size))\n",
    "\n",
    "    for row in range(rows):\n",
    "        for col in range(images_per_row):\n",
    "            channel_image = layer_activation[0, :, :, row * images_per_row + col]\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[row * size: (row + 1) * size, col * size: (col + 1) * size] = channel_image\n",
    "\n",
    "    # Mostrar las activaciones de la capa\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958d73f-3d54-4d1a-8533-838998ca8a33",
   "metadata": {},
   "source": [
    "## Fine-tuning Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1ef91-1cf8-49dc-9e5b-e5b0c689b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the VGG model\n",
    "vgg_conv = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224,224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0de0b-bc75-4944-b574-2610b4c0c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be587d",
   "metadata": {},
   "source": [
    "Para el proceso de Fine Tuning haremos entrenables las capas **block5_conv** que uniremos a nuestras capas finales de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_conv.trainable=True\n",
    "set_trainable=False\n",
    "\n",
    "# Freeze all the layers\n",
    "for layer in vgg_conv.layers[:]:\n",
    "  if layer.name == \"block5_conv1\":\n",
    "     set_trainable=True \n",
    "  if set_trainable:\n",
    "     layer.trainable=True\n",
    "  else: layer.trainable=False   \n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265725cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    "\n",
    "# Add new layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378eb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee10503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
